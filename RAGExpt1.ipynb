{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c364c097",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SelfQueryRetriever' from 'langchain.retrievers.self_query' (/home/nitish/Documents/github/RAGStudio/.venv/lib/python3.12/site-packages/langchain/retrievers/self_query/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_query\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiQueryRetriever\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontextual_compression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mself_query\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelfQueryRetriever\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_query\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiQueryRetriever\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontextual_compression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SelfQueryRetriever' from 'langchain.retrievers.self_query' (/home/nitish/Documents/github/RAGStudio/.venv/lib/python3.12/site-packages/langchain/retrievers/self_query/__init__.py)"
     ]
    }
   ],
   "source": [
    "# core LangChain 0.2+ components\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePick, RunnableParallel, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# handy utilities\n",
    "import os, pathlib\n",
    "import dotenv\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")  # or use dotenv / env vars\n",
    "\n",
    "# ollama\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aa35256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core LangChain interfaces (0.2+)\n",
    "from langchain_community.chat_models import ChatOllama  # local chat LLM\n",
    "from langchain_community.embeddings import OllamaEmbeddings  # local embeddings\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePick, RunnableLambda\n",
    "\n",
    "# Optional: LangSmith for tracing/eval\n",
    "from langsmith import Client\n",
    "import os, asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c155de4",
   "metadata": {},
   "source": [
    "### Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "270a74ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 185 0 (offset 0)\n",
      "Ignoring wrong pointing object 421 0 (offset 0)\n",
      "Ignoring wrong pointing object 423 0 (offset 0)\n",
      "Ignoring wrong pointing object 430 0 (offset 0)\n",
      "Ignoring wrong pointing object 643 0 (offset 0)\n",
      "Ignoring wrong pointing object 860 0 (offset 0)\n",
      "Ignoring wrong pointing object 892 0 (offset 0)\n",
      "Ignoring wrong pointing object 894 0 (offset 0)\n",
      "Ignoring wrong pointing object 928 0 (offset 0)\n",
      "Ignoring wrong pointing object 1047 0 (offset 0)\n",
      "Ignoring wrong pointing object 1124 0 (offset 0)\n",
      "Ignoring wrong pointing object 1138 0 (offset 0)\n",
      "Ignoring wrong pointing object 1148 0 (offset 0)\n",
      "Ignoring wrong pointing object 1173 0 (offset 0)\n",
      "Ignoring wrong pointing object 1380 0 (offset 0)\n",
      "Ignoring wrong pointing object 1405 0 (offset 0)\n",
      "Ignoring wrong pointing object 1468 0 (offset 0)\n",
      "Ignoring wrong pointing object 1520 0 (offset 0)\n",
      "Ignoring wrong pointing object 1578 0 (offset 0)\n",
      "Ignoring wrong pointing object 1670 0 (offset 0)\n",
      "Ignoring wrong pointing object 1673 0 (offset 0)\n",
      "Ignoring wrong pointing object 1762 0 (offset 0)\n",
      "Ignoring wrong pointing object 1879 0 (offset 0)\n",
      "Ignoring wrong pointing object 1894 0 (offset 0)\n",
      "Ignoring wrong pointing object 1913 0 (offset 0)\n",
      "Ignoring wrong pointing object 1978 0 (offset 0)\n",
      "Ignoring wrong pointing object 1996 0 (offset 0)\n",
      "Ignoring wrong pointing object 1998 0 (offset 0)\n",
      "Ignoring wrong pointing object 2180 0 (offset 0)\n",
      "Ignoring wrong pointing object 2182 0 (offset 0)\n",
      "Ignoring wrong pointing object 2186 0 (offset 0)\n",
      "Ignoring wrong pointing object 2190 0 (offset 0)\n",
      "Ignoring wrong pointing object 2229 0 (offset 0)\n",
      "Ignoring wrong pointing object 2277 0 (offset 0)\n",
      "Ignoring wrong pointing object 2306 0 (offset 0)\n",
      "Ignoring wrong pointing object 2332 0 (offset 0)\n",
      "Ignoring wrong pointing object 2676 0 (offset 0)\n",
      "Ignoring wrong pointing object 2750 0 (offset 0)\n",
      "Ignoring wrong pointing object 2897 0 (offset 0)\n",
      "Ignoring wrong pointing object 3267 0 (offset 0)\n",
      "Ignoring wrong pointing object 3318 0 (offset 0)\n",
      "Ignoring wrong pointing object 3539 0 (offset 0)\n",
      "Ignoring wrong pointing object 3558 0 (offset 0)\n",
      "Ignoring wrong pointing object 3961 0 (offset 0)\n",
      "Ignoring wrong pointing object 3963 0 (offset 0)\n",
      "Ignoring wrong pointing object 3978 0 (offset 0)\n",
      "Ignoring wrong pointing object 4082 0 (offset 0)\n",
      "Ignoring wrong pointing object 4084 0 (offset 0)\n",
      "Ignoring wrong pointing object 4088 0 (offset 0)\n",
      "Ignoring wrong pointing object 4791 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "# 1. ingest docs (10-Ks of JPMorgan as an example)\n",
    "loader = PyPDFLoader(\"docs/ISLP_website.pdf\")\n",
    "docs_raw = loader.load()\n",
    "\n",
    "# 2. split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=2750, chunk_overlap=600)\n",
    "docs_split = splitter.split_documents(docs_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bb37147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. embed & store in FAISS\n",
    "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3:latest\")\n",
    "vectordb = FAISS.from_documents(docs_split, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d4641a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. build a retriever\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 50})\n",
    "\n",
    "# 5. template\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert data scientist who loves to teach. You are very verbose and provide long explanations. You are trevor hastie. Use ONLY the context to answer.\",\n",
    "        ),\n",
    "        (\"human\", \"Context: <context>\\n{context}</context>\\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3:12b\")\n",
    "chain = (\n",
    "    RunnableParallel(context=retriever, question=RunnablePick(keys=[\"question\"]))\n",
    "    | template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cffacc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.invoke(\n",
    "    {\"question\": \"Explain linear regression in detail and its relation to LASSO\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e2a87ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down linear regression and its relationship to LASSO (Least Absolute Shrinkage and Selection Operator). I'm pulling information from the provided text to do this.\n",
      "\n",
      "**1. Linear Regression: The Basics**\n",
      "\n",
      "*   **What it is:** Linear regression is a supervised learning method primarily used to predict a *quantitative* (continuous) response variable. Think of it as a way to model the relationship between one or more predictor variables and a target variable that can take on a range of values.\n",
      "*   **Goal:**  The aim is to find the \"best-fitting\" line (or plane in multiple dimensions) that describes the relationship between the predictors and the response.\n",
      "*   **Example:** Predicting sales based on advertising budget (TV, radio, newspaper).  The linear regression model would attempt to find an equation that best explains how changes in advertising spending affect sales.\n",
      "*   **Why use it?**\n",
      "    *   It's a foundational technique – many more complex statistical learning methods are built upon it.\n",
      "    *   It's relatively simple to understand and implement.\n",
      "    *   It can help you determine if there's a relationship between variables (and how strong it is).\n",
      "*   **Limitations (when not to use it):**  It's not appropriate when the response variable is *qualitative* (categorical).  For example, predicting whether a patient has a stroke or not (instead of predicting the severity of a symptom, which would be quantitative).\n",
      "\n",
      "**2. LASSO (Least Absolute Shrinkage and Selection Operator)**\n",
      "\n",
      "*   **What it is:** LASSO is a regularization technique used in linear regression (and other models).  It's a way to improve the model's performance, especially when you have a lot of predictor variables (potentially with high multicollinearity).\n",
      "*   **How it works (the \"shrinkage\" and \"selection\" part):**\n",
      "    *   **Regularization:** LASSO adds a penalty term to the linear regression's ordinary least squares (OLS) objective function. This penalty term shrinks the coefficients of the predictor variables towards zero.  This helps prevent overfitting – where the model learns the noise in the training data rather than the underlying signal.\n",
      "    *   **Variable Selection:**  Unlike Ridge Regression (another regularization technique), LASSO can *force* some of the coefficients to be exactly zero. This effectively removes those predictor variables from the model. This \"variable selection\" property makes the resulting model simpler and easier to interpret.\n",
      "*   **The Math (briefly):**\n",
      "    *   LASSO uses an *L1 penalty* (sum of absolute values of coefficients) in the regularization term, whereas Ridge regression uses an *L2 penalty* (sum of squares of coefficients).  This seemingly small difference leads to the L1 penalty’s variable selection ability.\n",
      "*   **Benefits:**\n",
      "    *   **Simpler models:**  Fewer variables to consider, making interpretation easier.\n",
      "    *   **Improved generalization:**  Less prone to overfitting.\n",
      "    *   **Automatic variable selection:**  Helps identify the most important predictors.\n",
      "*   **Important Note:** You need to select a good value for the tuning parameter (often denoted as λ) – the amount of shrinkage. Cross-validation is commonly used for this.\n",
      "\n",
      "**The Relationship Between Linear Regression and LASSO**\n",
      "\n",
      "*   **LASSO *is* a modification of linear regression.** It builds upon the foundation of linear regression by adding a penalty term.\n",
      "*   **LASSO addresses some limitations of standard linear regression:**  Overfitting and difficulty in interpreting models with many predictors.\n",
      "*   **LASSO provides a way to perform automatic variable selection within a linear regression framework.**\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you'd like a deeper dive into any specific part of this explanation!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df27e90",
   "metadata": {},
   "source": [
    "### Filter + Re-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2c35fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SelfQueryRetriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m base = vectordb.as_retriever()\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# a) Self-Query: let GPT translate the user question into a vector search + metadata filter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m self_q = \u001b[43mSelfQueryRetriever\u001b[49m.from_llm(llm, base,\n\u001b[32m      5\u001b[39m            metadata_field_info=[(\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mfiscal_year\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m)])\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# b) Multi-Query: generate paraphrases of the question → union of results\u001b[39;00m\n\u001b[32m      8\u001b[39m multi_q = MultiQueryRetriever.from_llm(llm, base, num_queries=\u001b[32m4\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SelfQueryRetriever' is not defined"
     ]
    }
   ],
   "source": [
    "base = vectordb.as_retriever()\n",
    "\n",
    "# a) Self-Query: let GPT translate the user question into a vector search + metadata filter\n",
    "self_q = SelfQueryRetriever.from_llm(\n",
    "    llm, base, metadata_field_info=[(\"ticker\", \"str\"), (\"fiscal_year\", \"int\")]\n",
    ")\n",
    "\n",
    "# b) Multi-Query: generate paraphrases of the question → union of results\n",
    "multi_q = MultiQueryRetriever.from_llm(llm, base, num_queries=4)\n",
    "\n",
    "# c) Re-rank: apply bge-reranker or Cohere-rerank\n",
    "reranker = TopKReRank(k=4, model_name=\"bge-reranker-large\")\n",
    "\n",
    "# d) Compress: keep only the sentence fragments relevant to the query\n",
    "cc_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=multi_q, base_compressor=reranker\n",
    ")\n",
    "\n",
    "# use cc_retriever in the earlier chain\n",
    "chain2 = (\n",
    "    RunnableParallel(context=cc_retriever, question=RunnablePick(keys=[\"question\"]))\n",
    "    | template\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3547e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
